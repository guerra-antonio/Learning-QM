{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import qiskit.quantum_info as qi\n",
    "\n",
    "from torch import nn\n",
    "from codes import swapper, physical_imposition_operator, get_marginals, partialTrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For load the model previously trained is necessary to prepare the same architecture used in training stage\n",
    "\n",
    "Scale = 4\n",
    "\n",
    "class ConvDenoiser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvDenoiser, self).__init__()\n",
    "\n",
    "        # Encoder: Downsampling with convolutions and average pooling\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(2, Scale * 60, kernel_size = 3, padding = 1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(Scale * 60, Scale * 120, kernel_size = 3, padding = 1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(Scale * 120, Scale * 60, kernel_size = 3, padding = 1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Decoder: Upsampling with transpose convolutions\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(Scale * 60, Scale * 60, kernel_size = 3, padding = 1, stride = 2),\n",
    "            nn.Tanh(),\n",
    "\n",
    "            nn.ConvTranspose2d(Scale * 60, Scale * 120, kernel_size = 5, padding = 1, stride = 2),\n",
    "            nn.Tanh(),\n",
    "\n",
    "            nn.ConvTranspose2d(Scale * 120, Scale * 60, kernel_size = 6, stride = 2),\n",
    "            nn.Tanh(),\n",
    "\n",
    "            nn.Conv2d(Scale * 60, 2, kernel_size = 3)  # Final layer to return to 2 channels\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through encoder and decoder\n",
    "        encoded = self.encoder1(x)\n",
    "        output  = self.decoder1(encoded)\n",
    "        return output\n",
    "\n",
    "# Instantiate the model\n",
    "model = ConvDenoiser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvDenoiser(\n",
       "  (encoder1): Sequential(\n",
       "    (0): Conv2d(2, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Tanh()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (3): Conv2d(240, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Tanh()\n",
       "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (6): Conv2d(480, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): Tanh()\n",
       "    (8): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (decoder1): Sequential(\n",
       "    (0): ConvTranspose2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): Tanh()\n",
       "    (2): ConvTranspose2d(240, 480, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
       "    (3): Tanh()\n",
       "    (4): ConvTranspose2d(480, 240, kernel_size=(6, 6), stride=(2, 2))\n",
       "    (5): Tanh()\n",
       "    (6): Conv2d(240, 2, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load the model from the file checkpoint\n",
    "\n",
    "use_cpoint      = \"./checkpoint_N4_k3\"\n",
    "model_params    = torch.load(use_cpoint)\n",
    "\n",
    "model.load_state_dict(model_params['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the data that's going to be used for test stage\n",
    "\n",
    "dimension   = 2     # Dimension of each qudits, for qubits is 2.\n",
    "n_qubits    = 4     # Number of qubits of the global state\n",
    "n_marginal  = 3     # Number of marginal that we know\n",
    "n_matrices  = 500  # Number of matrices to be generated per rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To define the data is necessary to define a swapper matrix and label the differents marginal\n",
    "\n",
    "matrix_size = dimension ** n_qubits\n",
    "subs_dims   = [dimension for _ in range(n_qubits)]\n",
    "swapper_d   = swapper(dimension)\n",
    "labels_mg   = list(itertools.combinations(range(n_qubits), r = n_marginal))\n",
    "\n",
    "rho_noisless    = qi.random_density_matrix(dims = subs_dims).data\n",
    "test_mg_in      = get_marginals(rho_noisless, dimension, n_qubits, labels_mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvals of the noisy matrix =  \n",
      "\n",
      "   [-0.02990364 -0.00653641  0.00549846  0.01349185  0.02386722  0.03313472\n",
      "  0.04403347  0.05290858  0.06027106  0.06646289  0.08666911  0.0910665\n",
      "  0.10923285  0.13420924  0.15002503  0.16556904]\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Prepare the initial seed to implemente the physical imposition operator and, with that, get a matrix that contains the Quantum Marginals\n",
    "# but, not necessary, being a Quantum State\n",
    "\n",
    "initial_seed = qi.random_density_matrix(dims = subs_dims).data\n",
    "rho_noisy = physical_imposition_operator(dimension,n_qubits, initial_seed, test_mg_in, swapper_d)\n",
    "\n",
    "print(f\"Eigenvals of the noisy matrix =  \\n\\n   {np.linalg.eigvalsh(rho_noisy)}\")\n",
    "print(40*\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we replace the resulted matrix into our model to convert it into a density matrix\n",
    "\n",
    "input_state     = torch.Tensor(np.stack((rho_noisy.real, rho_noisy.imag)))\n",
    "output_state    = model(input_state.reshape(1, 2, matrix_size, matrix_size))\n",
    "predicted_state = output_state[0][0] + 1j*output_state[0][1]\n",
    "predicted_state = (predicted_state + torch.conj(predicted_state.T))/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check positivity\n",
    "\n",
    "The first step here is to check the conditions of the predicted matrix to be a quantum state, that is, check positivity. The normalization always can be done trought divide for the trace so, in general, even if the matrix do not have trace one we can renormalize to impose that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Eigenvalues of the predicted state = \n",
      "\n",
      "  [0.01543245 0.01752507 0.02005671 0.02542141 0.02706325 0.02842543\n",
      " 0.03674523 0.03974725 0.07169306 0.07376172 0.07995897 0.08596496\n",
      " 0.09794374 0.10479727 0.11585578 0.12273619]\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "sum of the eigenvals = 0.9631284940987825\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Normalization = (0.9631284475326538+0j)\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Eigenvals of the generator state =  \n",
      "\n",
      "   [0.00071919 0.00297635 0.00580721 0.00890401 0.01030026 0.0204045\n",
      " 0.03004263 0.03991976 0.05207595 0.06018732 0.07262291 0.0980476\n",
      " 0.10790311 0.12619917 0.17895469 0.18493533]\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We check that the output matrix is a Quantum state, that is, it is trace one and non-negative eigenvalues\n",
    "\n",
    "predicted_state = predicted_state.detach().numpy()\n",
    "eigenvalues     = np.linalg.eigvalsh(predicted_state)\n",
    "\n",
    "print(40*\"---\")\n",
    "print(f\"Eigenvalues of the predicted state = \\n\\n  {eigenvalues}\\n\")\n",
    "\n",
    "if not np.all(eigenvalues.real >= 0):\n",
    "    print(f\"Warning: negative eigenvalue found {np.min(eigenvalues.real)}\")\n",
    "    \n",
    "print(40*\"---\")\n",
    "print(f\"sum of the eigenvals = {sum(eigenvalues)}\\n\")\n",
    "print(40*\"---\")\n",
    "print(f\"Normalization = {np.trace(predicted_state)}\\n\")\n",
    "print(40*\"---\")\n",
    "print(f\"Eigenvals of the generator state =  \\n\\n   {np.linalg.eigvalsh(rho_noisless)}\")\n",
    "print(40*\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 2) 0.8645712186604758\n",
      "(0, 1, 3) 0.8874367430892801\n",
      "(0, 2, 3) 0.9038057530988741\n",
      "(1, 2, 3) 0.943073610590342\n"
     ]
    }
   ],
   "source": [
    "# The original problem is to get the global states that's contain the information about the quantum marginals, that is, if we get the marginals\n",
    "# from the neural network and, then, we compute the marginals trought partial trace and, then, compute the fidelity between the theoretical\n",
    "# marginals with the predicted ones the result should be close to 1.\n",
    "\n",
    "predicted_state = predicted_state/np.trace(predicted_state)\n",
    "predicted_mg    = get_marginals(predicted_state, dimension, n_qubits, labels_mg)\n",
    "\n",
    "for k in test_mg_in.keys():\n",
    "    u, v = test_mg_in[k], predicted_mg[k]\n",
    "    print(k, qi.state_fidelity(u, v, validate=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the actual fidelities are close to one. Better results can be obtained by a more trained model or a model trained with more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-body marginals\n",
    "\n",
    "Suppose a scenario where we just have marginals of two-bodies, that is, we just know quantum states of two-qubits and, with them, we want to compute the global system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target marginal [(1, 2)]: \n",
      "[[ 0.2701+0.j     -0.0256-0.0138j  0.0142+0.0276j  0.0194-0.0105j]\n",
      " [-0.0256+0.0138j  0.2412-0.j      0.015 -0.0315j  0.0083+0.0087j]\n",
      " [ 0.0142-0.0276j  0.015 +0.0315j  0.2266-0.j      0.0053+0.0013j]\n",
      " [ 0.0194+0.0105j  0.0083-0.0087j  0.0053-0.0013j  0.2621+0.j    ]]\n",
      "\n",
      "Predicted marginal [(1, 2)]:  \n",
      "[[ 0.238 +0.j      0.0103+0.0006j  0.0363+0.001j   0.0236-0.0005j]\n",
      " [ 0.0103-0.0006j  0.259 +0.j     -0.0008-0.0004j  0.0365+0.0018j]\n",
      " [ 0.0363-0.001j  -0.0008+0.0004j  0.2612+0.j      0.0138+0.0015j]\n",
      " [ 0.0236+0.0005j  0.0365-0.0018j  0.0138-0.0015j  0.2418+0.j    ]]\n",
      "\n",
      "Closeness: \n",
      "0.9871977233239974\n"
     ]
    }
   ],
   "source": [
    "# The first step is to label the marginals of the problem, for this case, suppose a marginal from the body 1 and 2.\n",
    "\n",
    "labels_mg   = [(1, 2)]\n",
    "test_mg     = get_marginals(rho_noisless, dimension, n_qubits, labels_marginals = labels_mg)\n",
    "pred_mg     = get_marginals(predicted_state, dimension, n_qubits, labels_marginals = labels_mg)\n",
    "\n",
    "print(f\"Target marginal {list(labels_mg)}: \")\n",
    "print(np.round(test_mg[labels_mg[0]], decimals=4))\n",
    "\n",
    "print(f\"\\nPredicted marginal {list(labels_mg)}:  \")\n",
    "print(np.round(pred_mg[labels_mg[0]], decimals=4))\n",
    "\n",
    "print(\"\\nCloseness: \")\n",
    "\n",
    "print(qi.state_fidelity(pred_mg[labels_mg[0]], test_mg[labels_mg[0]], validate=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, the Closeness computed by the fidelity is close to one, that's means we model correctly predict the quantum global state and, that quantum state, also posses the information of the marginals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
